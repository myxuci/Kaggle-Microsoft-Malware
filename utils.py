import gc
import time
from tqdm import tqdm
from numba import jit
import numpy as np
import pandas as pd
from contextlib import contextmanager

@jit
def filter_unique_ele_in_train(train, test, col, suffix='_filtered'):
    # assert col in train.columns and col in test.columns, ValueError('col is absent in either train or test.')
    new_col_name = col + suffix
    # print('Working on column of {}...'.format(col))
    test_col_unique_ele = test[col].unique().tolist()
    test_col_unique_ele_df = pd.DataFrame({col:test_col_unique_ele, new_col_name:test_col_unique_ele})
    output = train[col].to_frame().merge(test_col_unique_ele_df, how='left')

    # Double check:
    assert output.shape[0] == train.shape[0]

    return output, new_col_name

@jit
def change_unique_ele_in_train(train, test, vars_to_skip=['MachineIdentifier','HasDetections'], suffix='_filtered', inplace=True):
    _tic = time.time()
    vars_to_skip = [vars_to_skip] if not isinstance(vars_to_skip, list) else vars_to_skip
    for col in vars_to_skip:
        assert col in train.columns, ValueError('Column of {} does not exist in train...'.format(col))

    for col in tqdm(train.columns.tolist(), total=train.shape[1] - len(vars_to_skip)):
        if col in vars_to_skip:
            print('Skip column of {}...'.format(col))
            continue
        else:
            _proc_col_data, _new_col_name = filter_unique_ele_in_train(train, test, col, suffix=suffix)
            if inplace:
                train[col] = _proc_col_data[_new_col_name]
            else:
                train[_new_col_name] = _proc_col_data[_new_col_name]
                train.drop(col, axis=1, inplace=True)
            gc.collect()
    print('Time cost: {:.2f} minute(s)'.format((time.time()-_tic)/60.0))
    return train

def add_noise(series, noise_level):
    return series * (1 + noise_level * np.random.randn(len(series)))
def target_encode(trn_series=None, 
                  tst_series=None, 
                  target=None, 
                  min_samples_leaf=1, 
                  smoothing=1,
                  noise_level=0):
    """
    Smoothing is computed like in the following paper by Daniele Micci-Barreca
    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf
    trn_series : training categorical feature as a pd.Series
    tst_series : test categorical feature as a pd.Series
    target : target data as a pd.Series
    min_samples_leaf (int) : minimum samples to take category average into account
    smoothing (int) : smoothing effect to balance categorical average vs prior  
    """ 
    assert len(trn_series) == len(target)
    assert trn_series.name == tst_series.name
    temp = pd.concat([trn_series, target], axis=1)
    # Compute target mean 
    averages = temp.groupby(by=trn_series.name)[target.name].agg(["mean", "count"])
    # Compute smoothing
    smoothing = 1 / (1 + np.exp(-(averages["count"] - min_samples_leaf) / smoothing))
    # Apply average function to all target data
    prior = target.mean()
    # The bigger the count the less full_avg is taken into account
    averages[target.name] = prior * (1 - smoothing) + averages["mean"] * smoothing
    averages.drop(["mean", "count"], axis=1, inplace=True)
    # Apply averages to trn and tst series
    ft_trn_series = pd.merge(
        trn_series.to_frame(trn_series.name),
        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),
        on=trn_series.name,
        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)
    # pd.merge does not keep the index so restore it
    ft_trn_series.index = trn_series.index 
    ft_tst_series = pd.merge(
        tst_series.to_frame(tst_series.name),
        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),
        on=tst_series.name,
        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)
    # pd.merge does not keep the index so restore it
    ft_tst_series.index = tst_series.index
    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)

def print_warning(input_string, symbol='*'):
    assert isinstance(input_string, str) and isinstance(symbol, str), \
           TypeError('Illegal input formats.')
    print(symbol * len(input_string))
    print(input_string)
    print(symbol * len(input_string))

@contextmanager
def timer(title=None):
    if title is not None:
        assert isinstance(title, str), 'Title can only be strings.'
    else:
        title = 'THIS CHUCK OF CODES'

    _tic = time.time()
    yield
    gc.collect()
    _lag = time.time() - _tic

    if _lag <= 60:
        _unit = 'second(s)'
    elif (_lag > 60) & (_lag <= 3600):
        _unit = 'minute(s)'
        _lag /= 60
    else:
        _unit = 'hour(s)'
        _lag /= 3600

    print_warning('For {}, time cost: {:.2f} {}'.format(title.lower(), _lag, _unit))
    
def reduce_mem_usage(df, verbose=True):
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage(deep=True).sum() / 1024**2    
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)    
    end_mem = df.memory_usage(deep=True).sum() / 1024**2
    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))
    return df

@jit
def fast_auc(y_true, y_prob):
    y_true = np.asarray(y_true)
    y_true = y_true[np.argsort(y_prob)]
    nfalse = 0
    auc = 0
    n = len(y_true)
    for i in range(n):
        y_i = y_true[i]
        nfalse += (1 - y_i)
        auc += y_i * nfalse
    auc /= (nfalse * (n - nfalse))
    return auc

def eval_auc(preds, dtrain):
    labels = dtrain.get_label()
    return 'auc', fast_auc(labels, preds), True